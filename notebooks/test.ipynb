{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['marca', 'modelo', 'pais', 'year_modelo', 'clase', 'sub_clase',\n",
      "       'avaluo', 'cilindraje', 'tipo_combustible'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SEBPE\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.5286 - mae: 0.5064 - val_loss: 0.2180 - val_mae: 0.3111\n",
      "Epoch 2/35\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1643 - mae: 0.2638 - val_loss: 0.1877 - val_mae: 0.2762\n",
      "Epoch 3/35\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1132 - mae: 0.2163 - val_loss: 0.1941 - val_mae: 0.2758\n",
      "Epoch 4/35\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0868 - mae: 0.1916 - val_loss: 0.1844 - val_mae: 0.2641\n",
      "Epoch 5/35\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0814 - mae: 0.1853 - val_loss: 0.1824 - val_mae: 0.2658\n",
      "Epoch 6/35\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0735 - mae: 0.1720 - val_loss: 0.1840 - val_mae: 0.2734\n",
      "Epoch 7/35\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0699 - mae: 0.1676 - val_loss: 0.1815 - val_mae: 0.2610\n",
      "Epoch 8/35\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0668 - mae: 0.1641 - val_loss: 0.1828 - val_mae: 0.2611\n",
      "Epoch 9/35\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0619 - mae: 0.1561 - val_loss: 0.1770 - val_mae: 0.2582\n",
      "Epoch 10/35\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0643 - mae: 0.1600 - val_loss: 0.1839 - val_mae: 0.2660\n",
      "Epoch 11/35\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0646 - mae: 0.1558 - val_loss: 0.1830 - val_mae: 0.2614\n",
      "Epoch 12/35\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0624 - mae: 0.1545 - val_loss: 0.1824 - val_mae: 0.2575\n",
      "Epoch 13/35\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0570 - mae: 0.1485 - val_loss: 0.1761 - val_mae: 0.2568\n",
      "Epoch 14/35\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0599 - mae: 0.1489 - val_loss: 0.1785 - val_mae: 0.2603\n",
      "Epoch 15/35\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0615 - mae: 0.1477 - val_loss: 0.1795 - val_mae: 0.2556\n",
      "Epoch 16/35\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0573 - mae: 0.1459 - val_loss: 0.1820 - val_mae: 0.2599\n",
      "Epoch 17/35\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0590 - mae: 0.1464 - val_loss: 0.1953 - val_mae: 0.2710\n",
      "Epoch 18/35\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0559 - mae: 0.1442 - val_loss: 0.1812 - val_mae: 0.2586\n",
      "Epoch 19/35\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0530 - mae: 0.1380 - val_loss: 0.1796 - val_mae: 0.2563\n",
      "Epoch 20/35\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0610 - mae: 0.1469 - val_loss: 0.1763 - val_mae: 0.2513\n",
      "Epoch 21/35\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0502 - mae: 0.1345 - val_loss: 0.1832 - val_mae: 0.2611\n",
      "Epoch 22/35\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0487 - mae: 0.1335 - val_loss: 0.1804 - val_mae: 0.2555\n",
      "Epoch 23/35\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0512 - mae: 0.1380 - val_loss: 0.1786 - val_mae: 0.2543\n",
      "Epoch 24/35\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0507 - mae: 0.1325 - val_loss: 0.1823 - val_mae: 0.2559\n",
      "Epoch 25/35\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0528 - mae: 0.1377 - val_loss: 0.1806 - val_mae: 0.2617\n",
      "Epoch 26/35\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0499 - mae: 0.1339 - val_loss: 0.1885 - val_mae: 0.2591\n",
      "Epoch 27/35\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0514 - mae: 0.1352 - val_loss: 0.1777 - val_mae: 0.2500\n",
      "Epoch 28/35\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0498 - mae: 0.1317 - val_loss: 0.1819 - val_mae: 0.2566\n",
      "Epoch 29/35\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0457 - mae: 0.1244 - val_loss: 0.1818 - val_mae: 0.2541\n",
      "Epoch 30/35\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0474 - mae: 0.1273 - val_loss: 0.1798 - val_mae: 0.2540\n",
      "Epoch 31/35\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0489 - mae: 0.1278 - val_loss: 0.1817 - val_mae: 0.2566\n",
      "Epoch 32/35\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0489 - mae: 0.1279 - val_loss: 0.1874 - val_mae: 0.2582\n",
      "Epoch 33/35\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0475 - mae: 0.1279 - val_loss: 0.1770 - val_mae: 0.2515\n",
      "Epoch 34/35\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0477 - mae: 0.1290 - val_loss: 0.1827 - val_mae: 0.2523\n",
      "Epoch 35/35\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0470 - mae: 0.1256 - val_loss: 0.1798 - val_mae: 0.2557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modelo MLP entrenado y guardado correctamente.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# 📌 1. Cargar los datos\n",
    "file_path = \"../data/final/sri_autos_features.csv\"  # Ajusta la ruta\n",
    "#  Cargar el dataset procesado\n",
    "df = pd.read_csv(file_path, delimiter=';', encoding=\"latin-1\", low_memory=False)\n",
    "print(df.columns)\n",
    "\n",
    "# 📌 2. Seleccionar las columnas relevantes\n",
    "columnas_numericas = [\"year_modelo\", \"cilindraje\", \"avaluo\"]\n",
    "columnas_categoricas = [\"marca\", \"modelo\", \"pais\", \"clase\", \"sub_clase\", \"tipo_combustible\"]\n",
    "\n",
    "# 📌 3. Aplicar One-Hot Encoding a variables categóricas\n",
    "encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "encoded_data = encoder.fit_transform(df[columnas_categoricas])\n",
    "\n",
    "# 📌 4. Convertir One-Hot Encoding a DataFrame\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(columnas_categoricas))\n",
    "\n",
    "# 📌 5. Concatenar variables numéricas con las categóricas codificadas\n",
    "df_final = pd.concat([encoded_df, df[columnas_numericas]], axis=1)\n",
    "\n",
    "# 📌 6. Normalizar variables numéricas\n",
    "scaler = StandardScaler()\n",
    "df_final[columnas_numericas] = scaler.fit_transform(df_final[columnas_numericas])\n",
    "\n",
    "# 📌 7. Separar variables de entrada (X) y salida (y)\n",
    "X = df_final.drop(columns=[\"avaluo\"])\n",
    "y = df_final[\"avaluo\"]\n",
    "\n",
    "# 📌 8. Dividir en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 📌 9. Construir la red neuronal MLP\n",
    "modelo_mlp = Sequential([\n",
    "    Dense(128, activation=\"relu\", input_shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dense(32, activation=\"relu\"),\n",
    "    Dense(1)  # Predicción del precio\n",
    "])\n",
    "\n",
    "# 📌 10. Compilar el modelo\n",
    "modelo_mlp.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "# 📌 11. Entrenar el modelo\n",
    "modelo_mlp.fit(X_train, y_train, epochs=35, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# 📌 12. Guardar el modelo y los preprocesadores\n",
    "modelo_mlp.save(\"../models/mlp_model.h5\")\n",
    "joblib.dump(encoder, \"../models/mlp_encoder.pkl\")\n",
    "joblib.dump(scaler, \"../models/mlp_scaler.pkl\")\n",
    "\n",
    "print(\"✅ Modelo MLP entrenado y guardado correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\n",
      "📊 Evaluación del Modelo MLP:\n",
      "✅ MAE (Error Absoluto Medio): 0.26\n",
      "✅ RMSE (Raíz del Error Cuadrático Medio): 0.42\n",
      "✅ R² Score: 0.8189\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# 📌 1. Predecir en el conjunto de prueba\n",
    "y_pred = modelo_mlp.predict(X_test)\n",
    "\n",
    "# 📌 2. Evaluar el rendimiento del modelo\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\n📊 Evaluación del Modelo MLP:\")\n",
    "print(f\"✅ MAE (Error Absoluto Medio): {mae:.2f}\")\n",
    "print(f\"✅ RMSE (Raíz del Error Cuadrático Medio): {rmse:.2f}\")\n",
    "print(f\"✅ R² Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modelos y preprocesadores cargados correctamente.\n",
      "🔍 Columnas originales del encoder: ['marca' 'modelo' 'pais' 'clase' 'sub_clase' 'tipo_combustible']\n",
      "✅ One-Hot Encoding aplicado correctamente.\n",
      "❌ Error al normalizar datos: The feature names should match those that were passed during fit.\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- avaluo\n",
      "\n",
      "✅ Datos preparados con éxito. Dimensión final: (1, 3498)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step\n",
      "\n",
      "✅ Precio Predicho para el Auto: $595.73\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# 📌 Cargar el modelo y preprocesadores con manejo de errores\n",
    "try:\n",
    "    modelo_mlp = tf.keras.models.load_model(\n",
    "        \"../models/mlp_model.h5\",\n",
    "        custom_objects={\"mse\": tf.keras.losses.MeanSquaredError()}  # Solución al error de métricas\n",
    "    )\n",
    "    encoder = joblib.load(\"../models/mlp_encoder.pkl\")\n",
    "    scaler = joblib.load(\"../models/mlp_scaler.pkl\")\n",
    "    print(\"✅ Modelos y preprocesadores cargados correctamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error al cargar los modelos/preprocesadores: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 📌 Obtener las columnas originales del `encoder`\n",
    "try:\n",
    "    encoder_columns_original = encoder.feature_names_in_\n",
    "    print(f\"🔍 Columnas originales del encoder: {encoder_columns_original}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error al obtener las columnas originales del encoder: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 📌 Definir un nuevo auto para predecir su precio\n",
    "nuevo_auto = {\n",
    "    \"modelo\": \"TESLA MODEL Y\",\n",
    "    \"marca\": \"TESLA\",\n",
    "    \"pais\": \"ESTADOS UNIDOS\",\n",
    "    \"year_modelo\": 2023,\n",
    "    \"clase\": \"AUTOMOVIL\",\n",
    "    \"sub_clase\": \"ELECTRICO\",\n",
    "    \"cilindraje\": 0,\n",
    "    \"tipo_combustible\": \"ELECTRICO\"\n",
    "}\n",
    "\n",
    "# 📌 Convertir a DataFrame\n",
    "nuevo_auto_df = pd.DataFrame([nuevo_auto])\n",
    "\n",
    "# 📌 Usar las columnas categóricas originales del encoder\n",
    "columnas_categoricas = encoder_columns_original.tolist()\n",
    "columnas_numericas = [\"year_modelo\", \"cilindraje\"]  # Solo las numéricas usadas en `scaler`\n",
    "\n",
    "# 📌 Verificar que todas las columnas categóricas existen en `nuevo_auto_df`\n",
    "for col in columnas_categoricas:\n",
    "    if col not in nuevo_auto_df.columns:\n",
    "        print(f\"⚠️ Advertencia: La columna categórica '{col}' no está en el DataFrame.\")\n",
    "        exit()\n",
    "\n",
    "# 📌 Aplicar One-Hot Encoding con manejo de errores\n",
    "try:\n",
    "    encoded_nuevo_auto = encoder.transform(nuevo_auto_df[columnas_categoricas])\n",
    "    encoded_nuevo_auto_df = pd.DataFrame(encoded_nuevo_auto, columns=encoder.get_feature_names_out())\n",
    "    print(\"✅ One-Hot Encoding aplicado correctamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error en One-Hot Encoding: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 📌 Verificar que las columnas numéricas existen en `nuevo_auto_df`\n",
    "for col in columnas_numericas:\n",
    "    if col not in nuevo_auto_df.columns:\n",
    "        print(f\"⚠️ Advertencia: La columna numérica '{col}' no está en el DataFrame.\")\n",
    "        exit()\n",
    "\n",
    "# 📌 Asegurar que `scaler` no use `avaluo`\n",
    "try:\n",
    "    scaler_columns = scaler.feature_names_in_  # Verificamos con qué columnas se entrenó el `scaler`\n",
    "    columnas_validas_scaler = [col for col in columnas_numericas if col in scaler_columns]\n",
    "    nuevo_auto_df[columnas_validas_scaler] = scaler.transform(nuevo_auto_df[columnas_validas_scaler])\n",
    "    print(\"✅ Normalización aplicada correctamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error al normalizar datos: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 📌 Concatenar datos codificados y normalizados con verificación de tamaño\n",
    "try:\n",
    "    nuevo_auto_final = pd.concat([encoded_nuevo_auto_df, nuevo_auto_df[columnas_validas_scaler]], axis=1)\n",
    "    print(f\"✅ Datos preparados con éxito. Dimensión final: {nuevo_auto_final.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error al concatenar datos: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 📌 Verificar si la dimensión de `nuevo_auto_final` coincide con la entrada del modelo\n",
    "expected_features = modelo_mlp.input_shape[1]\n",
    "if nuevo_auto_final.shape[1] != expected_features:\n",
    "    print(f\"❌ Error: Dimensión incorrecta. Se esperaban {expected_features} columnas, pero se obtuvieron {nuevo_auto_final.shape[1]}.\")\n",
    "    exit()\n",
    "\n",
    "# 📌 Asegurar la forma correcta de la entrada\n",
    "entrada_modelo = nuevo_auto_final.to_numpy().reshape(1, -1)\n",
    "\n",
    "# 📌 Predecir el precio del auto\n",
    "try:\n",
    "    predicted_price = modelo_mlp.predict(entrada_modelo)[0][0]\n",
    "    print(f\"\\n✅ Precio Predicho para el Auto: ${predicted_price:.2f}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error al realizar la predicción: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
